{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000b4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd67057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from open_clip import tokenizer\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8f73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/cluster/scratch/nkoisheke/artstation_art_data_merged.csv\"\n",
    "text = []\n",
    "i = 0\n",
    "artist_names = []\n",
    "with open(path, 'r') as f:\n",
    "    for r in f:\n",
    "        if i > 0:\n",
    "            artist_names.append(r.split(\"\\t\")[0])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0159928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7618\n"
     ]
    }
   ],
   "source": [
    "print(len(artist_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9482dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7618/7618 [00:03<00:00, 2212.76it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "new_artist_names = []\n",
    "# change the image folders here\n",
    "path = \"/cluster/scratch/nkoisheke/artworks\"\n",
    "number_of_images = len(os.listdir(path))\n",
    "for i in tqdm(range(len(artist_names))):\n",
    "    temp_path = os.path.join(path, f\"index_{i}.png\") \n",
    "    if os.path.exists(temp_path):\n",
    "        name = artist_names[i]\n",
    "        if name.isascii():\n",
    "            img = PIL.Image.open(temp_path)\n",
    "            temp_img = img.copy()\n",
    "            img.close()\n",
    "            temp_img = preprocess(temp_img)\n",
    "            data.append(temp_img)\n",
    "            # new_artist_names.append(name)\n",
    "            # data.append(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91662cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_paths = [d.split(\"/\")[-1] for d in data]\n",
    "# import pandas as pd\n",
    "\n",
    "# new_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"uid\": [i for i in range(len(new_artist_names))],\n",
    "#         \"artist\": new_artist_names,\n",
    "#         \"image_name\": img_paths,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# new_df.to_csv(\"/cluster/scratch/nkoisheke/real_images_artstation_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c221b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "972c4433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "image_features = []\n",
    "batch_size = 512\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = []\n",
    "        for j in range(i, min(i+batch_size, len(data))):\n",
    "            batch.append(data[j])\n",
    "        batch = torch.tensor(np.stack(batch))\n",
    "        batch = batch.to(device)\n",
    "        img_feats = model.encode_image(batch).float()\n",
    "        image_features.append(img_feats)\n",
    "image_features_torch = torch.concatenate(image_features).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2aeccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7031, 512])\n"
     ]
    }
   ],
   "source": [
    "print(image_features_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "603800f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "617d45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     image_features_torch, \n",
    "#     '/cluster/scratch/nkoisheke/real_images_artstation_filtered_' + 'laion2b_s34b_b79k' + '_ViT-B-32' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d279178",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = list(set(new_artist_names))\n",
    "text_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for artist in artists:\n",
    "        prompt = f\"Done in the style of {artist} on ArtStation.\"\n",
    "        text_tokens = tokenizer.tokenize(prompt)\n",
    "        text_tokens = text_tokens.to(device)\n",
    "        txt_feat = model.encode_text(text_tokens).float()\n",
    "        text_features.append(txt_feat)\n",
    "text_features_torch = torch.concatenate(text_features).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3971d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2artist = {}\n",
    "artist2idx = {}\n",
    "for i, artist in enumerate(artists):\n",
    "    idx2artist[i] = artist\n",
    "    artist2idx[artist] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d30c14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_torch /= image_features_torch.norm(dim=-1, keepdim=True)\n",
    "text_features_torch /= text_features_torch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_probs = (100.0 * image_features_torch @ text_features_torch.T).softmax(dim=-1)\n",
    "top_probs, top_k_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80aa9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7031]) torch.Size([7031])\n",
      "Top 1 score is 2.03\n",
      "Top 5 score is 6.0\n"
     ]
    }
   ],
   "source": [
    "gt_labels = torch.tensor([artist2idx[x] for x in new_artist_names])\n",
    "\n",
    "top_one_labels = top_k_labels[:, 0]\n",
    "\n",
    "print(gt_labels.shape, top_one_labels.shape)\n",
    "\n",
    "correct = (gt_labels == top_one_labels).sum()\n",
    "print(f\"Top 1 score is {round((correct / gt_labels.shape[0]).item() * 100, 2)}\")\n",
    "\n",
    "\n",
    "topk_correct = 0\n",
    "for i in range(5):\n",
    "    top_one_labels = top_k_labels[:, i]\n",
    "    correct = (gt_labels == top_one_labels).sum()\n",
    "    topk_correct += correct\n",
    "print(f\"Top 5 score is {round((topk_correct / gt_labels.shape[0]).item() * 100, 2)}\")\n",
    "# correct = (gt_labels.repeat() == top_k_labels).sum()\n",
    "# print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f1b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edd03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
